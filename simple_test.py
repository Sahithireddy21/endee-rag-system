"""Simple test of RAG components without heavy dependencies."""

import json
import urllib.request
import urllib.parse
from pathlib import Path

def test_endee_basic_operations():
    """Test basic Endee operations using only standard library."""
    print("üîß Testing Endee Basic Operations")
    print("=" * 40)
    
    base_url = "http://localhost:8080"
    
    # Test 1: Health Check
    print("1. Health Check...")
    try:
        with urllib.request.urlopen(f"{base_url}/health", timeout=5) as response:
            if response.status == 200:
                print("   ‚úÖ Endee is healthy")
            else:
                print(f"   ‚ùå Health check failed: {response.status}")
                return False
    except Exception as e:
        print(f"   ‚ùå Cannot connect to Endee: {e}")
        return False
    
    # Test 2: List Collections
    print("2. List Collections...")
    try:
        with urllib.request.urlopen(f"{base_url}/collections", timeout=5) as response:
            if response.status == 200:
                data = json.loads(response.read().decode())
                print(f"   ‚úÖ Collections endpoint accessible")
                print(f"   üìã Response: {data}")
            else:
                print(f"   ‚ö†Ô∏è  Collections endpoint returned: {response.status}")
    except Exception as e:
        print(f"   ‚ùå Error accessing collections: {e}")
    
    # Test 3: Create a Test Collection
    print("3. Create Test Collection...")
    try:
        collection_data = {
            "name": "test_collection",
            "dimension": 384,
            "metric": "cosine"
        }
        
        data = json.dumps(collection_data).encode('utf-8')
        req = urllib.request.Request(
            f"{base_url}/collections",
            data=data,
            headers={'Content-Type': 'application/json'}
        )
        
        with urllib.request.urlopen(req, timeout=10) as response:
            if response.status in [200, 201]:
                print("   ‚úÖ Test collection created successfully")
            elif response.status == 409:
                print("   ‚úÖ Test collection already exists")
            else:
                print(f"   ‚ö†Ô∏è  Create collection returned: {response.status}")
                
    except Exception as e:
        print(f"   ‚ùå Error creating collection: {e}")
    
    # Test 4: Insert Test Vector
    print("4. Insert Test Vector...")
    try:
        # Create a simple test vector (384 dimensions with random-ish values)
        test_vector = [0.1 * i for i in range(384)]  # Simple test vector
        
        vector_data = {
            "vectors": [{
                "id": "test_vector_1",
                "vector": test_vector,
                "metadata": {
                    "text": "This is a test document chunk",
                    "source": "test.txt",
                    "page": 1
                }
            }]
        }
        
        data = json.dumps(vector_data).encode('utf-8')
        req = urllib.request.Request(
            f"{base_url}/collections/test_collection/vectors",
            data=data,
            headers={'Content-Type': 'application/json'}
        )
        
        with urllib.request.urlopen(req, timeout=10) as response:
            if response.status in [200, 201]:
                print("   ‚úÖ Test vector inserted successfully")
            else:
                print(f"   ‚ö†Ô∏è  Insert vector returned: {response.status}")
                
    except Exception as e:
        print(f"   ‚ùå Error inserting vector: {e}")
    
    # Test 5: Search Test Vector
    print("5. Search Test Vector...")
    try:
        # Use the same vector for search (should return itself with high similarity)
        search_data = {
            "vector": test_vector,
            "top_k": 3,
            "threshold": 0.0
        }
        
        data = json.dumps(search_data).encode('utf-8')
        req = urllib.request.Request(
            f"{base_url}/collections/test_collection/search",
            data=data,
            headers={'Content-Type': 'application/json'}
        )
        
        with urllib.request.urlopen(req, timeout=10) as response:
            if response.status == 200:
                results = json.loads(response.read().decode())
                print("   ‚úÖ Vector search successful")
                print(f"   üìä Found {len(results.get('results', []))} results")
                
                if results.get('results'):
                    for i, result in enumerate(results['results'][:2]):
                        score = result.get('score', 0)
                        metadata = result.get('metadata', {})
                        text = metadata.get('text', 'No text')
                        print(f"      {i+1}. Score: {score:.3f} - {text[:50]}...")
            else:
                print(f"   ‚ö†Ô∏è  Search returned: {response.status}")
                
    except Exception as e:
        print(f"   ‚ùå Error searching vectors: {e}")
    
    return True

def test_pdf_processing_simulation():
    """Simulate PDF processing without actual PDF libraries."""
    print("\nüìÑ Simulating PDF Processing")
    print("=" * 40)
    
    # Simulate extracted text from a PDF
    sample_text = """
    Machine Learning Overview
    
    Machine learning is a subset of artificial intelligence (AI) that enables computers 
    to learn and make decisions from data without being explicitly programmed for every task.
    
    Key Concepts:
    1. Supervised Learning: Learning with labeled examples
    2. Unsupervised Learning: Finding patterns in unlabeled data  
    3. Neural Networks: Computing systems inspired by biological neural networks
    4. Deep Learning: Neural networks with multiple layers
    
    Applications include natural language processing, computer vision, robotics, 
    and predictive analytics. These technologies are transforming industries by 
    enabling automation and intelligent decision-making.
    """
    
    print("1. Simulated Text Extraction...")
    print(f"   ‚úÖ Extracted {len(sample_text)} characters")
    
    print("2. Text Chunking...")
    # Simple chunking by sentences
    sentences = [s.strip() for s in sample_text.split('.') if s.strip()]
    
    chunks = []
    current_chunk = ""
    chunk_id = 0
    
    for sentence in sentences:
        if len(current_chunk) + len(sentence) < 200:  # Simple chunk size limit
            current_chunk += sentence + ". "
        else:
            if current_chunk:
                chunks.append({
                    "chunk_id": chunk_id,
                    "text": current_chunk.strip(),
                    "metadata": {
                        "source": "sample_ml_doc.pdf",
                        "page": 1,
                        "chunk_index": chunk_id
                    }
                })
                chunk_id += 1
            current_chunk = sentence + ". "
    
    # Don't forget the last chunk
    if current_chunk:
        chunks.append({
            "chunk_id": chunk_id,
            "text": current_chunk.strip(),
            "metadata": {
                "source": "sample_ml_doc.pdf", 
                "page": 1,
                "chunk_index": chunk_id
            }
        })
    
    print(f"   ‚úÖ Created {len(chunks)} chunks")
    
    print("3. Simulated Embedding Generation...")
    # Simulate embeddings (in real implementation, this would use OpenAI)
    for chunk in chunks:
        # Create a simple "embedding" based on text length and content
        text_hash = hash(chunk["text"]) % 1000
        chunk["embedding"] = [0.001 * (text_hash + i) for i in range(384)]
        chunk["embedding_model"] = "simulated-embedding"
    
    print(f"   ‚úÖ Generated embeddings for {len(chunks)} chunks")
    
    print("4. Sample Chunks:")
    for i, chunk in enumerate(chunks[:3]):
        print(f"   Chunk {i+1}: {chunk['text'][:80]}...")
    
    return chunks

def test_search_simulation(chunks):
    """Simulate document search."""
    print("\nüîç Simulating Document Search")
    print("=" * 40)
    
    # Simulate a user query
    query = "What is machine learning?"
    print(f"Query: '{query}'")
    
    # Simple keyword-based similarity (in real implementation, this would use embeddings)
    query_words = set(query.lower().split())
    
    results = []
    for chunk in chunks:
        chunk_words = set(chunk["text"].lower().split())
        # Simple Jaccard similarity
        intersection = len(query_words.intersection(chunk_words))
        union = len(query_words.union(chunk_words))
        similarity = intersection / union if union > 0 else 0
        
        if similarity > 0:
            results.append({
                "chunk": chunk,
                "similarity": similarity
            })
    
    # Sort by similarity
    results.sort(key=lambda x: x["similarity"], reverse=True)
    
    print(f"‚úÖ Found {len(results)} relevant chunks")
    
    print("Top Results:")
    for i, result in enumerate(results[:3]):
        chunk = result["chunk"]
        similarity = result["similarity"]
        print(f"   {i+1}. Similarity: {similarity:.3f}")
        print(f"      Text: {chunk['text'][:100]}...")
        print()

def main():
    """Run all tests."""
    print("üß™ RAG System Component Tests")
    print("=" * 50)
    
    # Test Endee operations
    endee_working = test_endee_basic_operations()
    
    if endee_working:
        print("\n‚úÖ Endee is working correctly!")
    else:
        print("\n‚ùå Endee tests failed. Make sure it's running with: docker-compose up endee")
        return
    
    # Test document processing simulation
    chunks = test_pdf_processing_simulation()
    
    # Test search simulation
    test_search_simulation(chunks)
    
    print("\n" + "=" * 50)
    print("üéâ All basic tests completed!")
    print("\nüìã What's Working:")
    print("‚úÖ Endee vector database connection")
    print("‚úÖ Collection creation and management")
    print("‚úÖ Vector insertion and search")
    print("‚úÖ Document processing simulation")
    print("‚úÖ Search functionality simulation")
    
    print("\nüöÄ Ready for Full Implementation!")
    print("Next: Install full dependencies and run with real OpenAI embeddings")

if __name__ == "__main__":
    main()